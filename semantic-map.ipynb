{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook uses Python. Run cells in order from top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Edit the values in the cell below to match your data **before running the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION â€” Edit these values before running\n",
    "# =============================================================\n",
    "\n",
    "# Path to directory containing .txt files (one document per file)\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# --- Required: at least one text column ---\n",
    "TEXT_COL_PRIMARY   = \"text\"   # Content of each .txt file\n",
    "TEXT_COL_SECONDARY = None     # No secondary column for plain text files\n",
    "\n",
    "# --- Optional metadata columns ---\n",
    "# Set any of these to None if your data doesn't include them\n",
    "DATE_COL   = None  # No date column\n",
    "DOMAIN_COL = None  # No domain column\n",
    "URL_COL    = None  # No URL column\n",
    "\n",
    "# --- Deduplication ---\n",
    "# Columns to deduplicate on â€” set to [] to skip deduplication entirely\n",
    "DEDUPE_COLS = []\n",
    "\n",
    "# --- Output ---\n",
    "OUTPUT_DIR = \"output\"   # Folder to save results (created automatically if it doesn't exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load Data & Remove Duplicates ðŸ§¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Option A: Load from a CSV file ---\n",
    "# df = pd.read_csv(\"data/your_file.csv\")\n",
    "# print(f\"Loaded {len(df):,} rows from CSV\")\n",
    "\n",
    "# --- Option B: Load data from .txt files â€” one document per file ---\n",
    "records = []\n",
    "for fname in sorted(os.listdir(DATA_DIR)):\n",
    "    if fname.endswith(\".txt\"):\n",
    "        fpath = os.path.join(DATA_DIR, fname)\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip()\n",
    "        records.append({\"filename\": os.path.splitext(fname)[0], \"text\": content})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Loaded {len(df):,} documents from '{DATA_DIR}'\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "Rows that will be removed as duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEDUPE_COLS:\n",
    "    dupes = df[df.duplicated(subset=DEDUPE_COLS, keep=False)].sort_values(by=DEDUPE_COLS)\n",
    "    print(f\"{len(dupes):,} duplicate rows found (columns used: {DEDUPE_COLS})\")\n",
    "    display(dupes.head())\n",
    "else:\n",
    "    print(\"DEDUPE_COLS is empty â€” skipping deduplication preview.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEDUPE_COLS:\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=DEDUPE_COLS, keep=\"last\").reset_index(drop=True)\n",
    "    print(f\"Removed {before - len(df):,} duplicates. {len(df):,} rows remaining.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Generate a vector embedding for each row. **Run Option A *or* Option B â€” not both.**\n",
    "\n",
    "| | Option A | Option B |\n",
    "|---|---|---|\n",
    "| **Model** | `all-MiniLM-L6-v2` (local) | `text-embedding-3-small` (OpenAI) |\n",
    "| **Cost** | Free | ~$0.02 / million tokens |\n",
    "| **Requires** | Nothing extra | API key in `.env` |\n",
    "| **Download** | ~80 MB (first run only) | None |\n",
    "| **Quality** | Good enough for clustering | Higher |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "ENCODING   = \"cl100k_base\"\n",
    "MAX_TOKENS = 8000\n",
    "enc        = tiktoken.get_encoding(ENCODING)\n",
    "\n",
    "def make_combined_text(row):\n",
    "    \"\"\"Combine primary and (optional) secondary text columns for embedding.\"\"\"\n",
    "    primary = str(row[TEXT_COL_PRIMARY]) if pd.notna(row[TEXT_COL_PRIMARY]) else \"\"\n",
    "    if (TEXT_COL_SECONDARY\n",
    "            and TEXT_COL_SECONDARY in row.index\n",
    "            and pd.notna(row[TEXT_COL_SECONDARY])):\n",
    "        secondary = str(row[TEXT_COL_SECONDARY])\n",
    "        return f\"Title: {primary}; Content: {secondary}\"\n",
    "    return primary\n",
    "\n",
    "df[\"combined\"] = df.apply(make_combined_text, axis=1)\n",
    "df[\"n_tokens\"] = df[\"combined\"].apply(lambda x: len(enc.encode(x)))\n",
    "df = df.sort_values(\"n_tokens\", ascending=False)\n",
    "print(f\"Longest article: {df['n_tokens'].max():,} tokens | Average: {df['n_tokens'].mean():.0f} tokens\")\n",
    "df[[\"combined\", \"n_tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "too_long = df[df[\"n_tokens\"] > MAX_TOKENS]\n",
    "print(f\"Removing {len(too_long):,} articles exceeding {MAX_TOKENS:,} tokens.\")\n",
    "df = df[df[\"n_tokens\"] <= MAX_TOKENS].reset_index(drop=True)\n",
    "print(f\"{len(df):,} rows remaining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-opt-a-md",
   "metadata": {},
   "source": [
    "### Option A â€” Local model\n",
    "\n",
    "No API key needed. Model downloads ~80 MB on first run, then works offline.\n",
    "\n",
    "> Skip to Option B if you prefer to use the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-opt-b-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # â”€â”€ Option A: local model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # downloads ~80 MB on first run\n",
    "\n",
    "# df[\"embedding\"] = model.encode(\n",
    "#     df[\"combined\"].tolist(),\n",
    "#     show_progress_bar=True,\n",
    "#     batch_size=64,\n",
    "# ).tolist()\n",
    "\n",
    "# print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-opt-b-md",
   "metadata": {},
   "source": [
    "### Option B â€” OpenAI API\n",
    "\n",
    "Higher quality embeddings. Requires an API key in `.env` and the `openai` + `python-dotenv` packages.\n",
    "\n",
    "> Skip this cell if you already ran Option A above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Option B only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# â”€â”€ Option B: OpenAI embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_embeddings(texts):\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=texts)\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "def process_in_batches(df, column, batch_size=30):\n",
    "    all_embeddings = []\n",
    "    texts = df[column].tolist()\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        all_embeddings.extend(get_embeddings(batch))\n",
    "    return all_embeddings\n",
    "\n",
    "df[\"embedding\"] = process_in_batches(df, \"combined\")\n",
    "\n",
    "df = df.drop(columns=[\"combined\", \"n_tokens\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (t-SNE)\n",
    "\n",
    "Embeddings are high-dimensional vectors (~1,536 dimensions). t-SNE projects them to 2D (x, y) for plotting.\n",
    "This step is cached â€” re-running the notebook will load saved coordinates instead of recomputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_cache = os.path.join(OUTPUT_DIR, \"tsne-cache.csv\")\n",
    "\n",
    "if os.path.exists(tsne_cache):\n",
    "    print(f\"Loading cached t-SNE coordinates from '{tsne_cache}'\")\n",
    "    coords = pd.read_csv(tsne_cache)\n",
    "    df[\"x\"] = coords[\"x\"].values\n",
    "    df[\"y\"] = coords[\"y\"].values\n",
    "else:\n",
    "    print(\"Running t-SNE (this may take a few minutes on large datasets)...\")\n",
    "    matrix = np.array(df[\"embedding\"].tolist())\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=min(30, len(df) // 5),  # scales with dataset size\n",
    "        random_state=42,\n",
    "        init=\"pca\",           # stable, deterministic\n",
    "        learning_rate=\"auto\") # adapts to dataset size\n",
    "    vis_dims = tsne.fit_transform(matrix)\n",
    "    df[\"x\"] = vis_dims[:, 0]\n",
    "    df[\"y\"] = vis_dims[:, 1]\n",
    "    df[[\"x\", \"y\"]].to_csv(tsne_cache, index=False)\n",
    "    print(f\"Done. Coordinates cached to '{tsne_cache}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Topic Modeling (DBSCAN)\n",
    "\n",
    "[DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) clusters points in vector space â€” articles in the same cluster tend to be about the same topic.\n",
    "\n",
    "- **Topic 0** = noise (articles that don't fit into any cluster)\n",
    "- Adjust `DBSCAN_EPS` and `DBSCAN_MIN_SAMPLES` in the Configuration cell if you get too few or too many clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clustering parameters (cosine distance, range 0â€“2) ---\n",
    "# eps:         max cosine distance between two points in the same cluster\n",
    "#              lower = tighter clusters (e.g. 0.2); higher = looser (e.g. 0.5)\n",
    "# min_samples: minimum number of documents required to form a cluster\n",
    "DBSCAN_EPS         = 0.5\n",
    "DBSCAN_MIN_SAMPLES = 2\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "matrix = np.array(df[\"embedding\"].tolist())\n",
    "\n",
    "# Diagnostic: show distance distribution to help tune eps\n",
    "dists = cosine_distances(matrix)\n",
    "np.fill_diagonal(dists, np.nan)\n",
    "flat = dists[~np.isnan(dists)]\n",
    "print(\"Cosine distance percentiles (use these to set DBSCAN_EPS):\")\n",
    "for p in [10, 25, 50, 75, 90]:\n",
    "    print(f\"  {p}th percentile: {np.percentile(flat, p):.3f}\")\n",
    "\n",
    "labels = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES, metric=\"cosine\").fit_predict(matrix)\n",
    "df[\"topic\"] = labels + 1   # shift so DBSCAN noise (-1) becomes topic 0\n",
    "\n",
    "topic_counts = (df.groupby(\"topic\")\n",
    "                  .size()\n",
    "                  .reset_index(name=\"count\")\n",
    "                  .sort_values(\"count\", ascending=False))\n",
    "n_clusters = (topic_counts[\"topic\"] > 0).sum()\n",
    "n_noise    = topic_counts.loc[topic_counts[\"topic\"] == 0, \"count\"].sum()\n",
    "print(f\"\\nFound {n_clusters} clusters + {n_noise:,} uncategorized (noise) articles\")\n",
    "\n",
    "df[\"topic_label\"] = df[\"topic\"].apply(\n",
    "    lambda t: \"Uncategorized\" if t == 0 else f\"Topic {t}\"\n",
    ")\n",
    "\n",
    "summary = (df.groupby([\"topic\", \"topic_label\"])\n",
    "             .size()\n",
    "             .reset_index(name=\"count\")\n",
    "             .sort_values(\"count\", ascending=False))\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full dataset (all columns, including embedding vectors) ---\n",
    "full_path = os.path.join(OUTPUT_DIR, \"data-with-embeddings.csv\")\n",
    "df.to_csv(full_path, index=False)\n",
    "print(f\"Full dataset    â†’ '{full_path}'\")\n",
    "\n",
    "# --- Visualization dataset ---\n",
    "# Renames columns to match the visualization tool's expected schema:\n",
    "#   title, text, date, org, x, y, url\n",
    "rename_map = {}\n",
    "if TEXT_COL_PRIMARY:   rename_map[TEXT_COL_PRIMARY]   = \"title\"\n",
    "if TEXT_COL_SECONDARY: rename_map[TEXT_COL_SECONDARY] = \"text\"\n",
    "if DATE_COL:           rename_map[DATE_COL]           = \"date\"\n",
    "if DOMAIN_COL:         rename_map[DOMAIN_COL]         = \"org\"\n",
    "if URL_COL:            rename_map[URL_COL]            = \"url\"\n",
    "\n",
    "vis_df = df.rename(columns=rename_map)\n",
    "\n",
    "vis_cols = [c for c in [\"title\", \"text\", \"date\", \"org\", \"x\", \"y\", \"url\", \"topic_label\"]\n",
    "            if c in vis_df.columns]\n",
    "\n",
    "vis_path = os.path.join(OUTPUT_DIR, \"data-for-visualization.csv\")\n",
    "vis_df[vis_cols].to_csv(vis_path, index=False)\n",
    "print(f\"Visualization   â†’ '{vis_path}'\")\n",
    "print(f\"\\nColumns exported: {vis_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5k17djx1",
   "metadata": {},
   "source": [
    "## Interactive Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdk7atic3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import textwrap\n",
    "\n",
    "def wrap_hover(text, width=60):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    lines = textwrap.wrap(str(text), width=width)\n",
    "    return \"<br>\".join(lines)\n",
    "\n",
    "plot_df = df.copy()\n",
    "plot_df[\"_primary\"] = plot_df[TEXT_COL_PRIMARY].apply(lambda t: wrap_hover(t, width=60))\n",
    "\n",
    "hover_name_col = \"filename\" if \"filename\" in df.columns else TEXT_COL_PRIMARY\n",
    "hover_data = {\"_primary\": True, \"x\": False, \"y\": False}\n",
    "if TEXT_COL_SECONDARY and TEXT_COL_SECONDARY in df.columns:\n",
    "    plot_df[\"_secondary\"] = plot_df[TEXT_COL_SECONDARY].apply(lambda t: wrap_hover(t, width=60))\n",
    "    hover_data[\"_secondary\"] = True\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"topic_label\",\n",
    "    hover_name=hover_name_col,\n",
    "    hover_data=hover_data,\n",
    "    title=\"Semantic Map\",\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "fig.update_traces(marker=dict(size=9, opacity=0.7))\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"Topic\",\n",
    "    hoverlabel=dict(namelength=-1),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
